{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A program is said to \"learn\" from experience E if the task T's performance measure P imporves with experience E.<br>\n",
    "data,answers--->[MODEL]---> rules<br>\n",
    "Input,Output--->[MODEL]--->weights/parameters\n",
    "\n",
    "Model/Hypothesis: <br>\n",
    "$h_w(x)=w_0+w_1x,    w_i s - weight$<br>\n",
    "Supervised Learning- form is given by the user and weights are determined by the machine.<br>\n",
    "**Input Feature**: (x,y)<br>\n",
    "**Hypothesis/Model**: h(w)<br>\n",
    "**Error/cost/loss Function**: J(w)<br>\n",
    "\n",
    "Supervised, Unsupervised-no labelling, Semisuper\n",
    "<br>\n",
    "Predication/Classification problem<br>\n",
    "ABC dataset A Big Cadmodel data set\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (Prediction)\n",
    "m: sample/traning size<br>\n",
    "Hypothese $h_w(x)=w_0+w_1x$ <br>\n",
    "$w_0:$ bias and w_is:parameters<br>\n",
    "superscript notation denotes sample number $x^k$<br>\n",
    "Every x we have a ground truth and predicted output<br>\n",
    "Take distance norm $(y-y_0)^2$<br>\n",
    "Cost function: $J(y,Y_0)=\\sum 1/2m (y-y_0)^2$<br>\n",
    "$J(y,Y_0)=\\sum 1/2m (w_0+w_1x^{(i)}-y_0^{(i)})^2$<br>\n",
    "$\\triangledown J = dJ/d(w_1),dJ/d(w_1)$ _vector_<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the cost function- plotting J(w0,w1) and find minima through gradient descent.<br>\n",
    "$\\triangledown J= (dJ/w0,dJ/w1)$<br>\n",
    "$dJ(w_1,w_0)/dw_0=\\sum 1/m (w_0x^{(0)}+w_1x^{(i)}-y_0^{(i)})x^{(0)}$<br>\n",
    "$dJ(w_1,w_0)/dw_1=\\sum 1/m (w_0x^{(0)}+w_1x^{(i)}-y_0^{(i)})x^{(i)}$<br>\n",
    "$x^{(0)}$ is just used to give a more symmetric form, here it is equal to 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate case<br>\n",
    "J(x1,x1...xn,y) there are n features<br>\n",
    "if we have m samples of training data:<br>\n",
    "$(x1^{(1)},x1^{(2)}...x1^{(m)})$<br>\n",
    "Every feature has m samples, and every sample has n coordinates.<br>\n",
    "Our hypothesis will have n+1 values<br>\n",
    "$h_w= w_0x_0+w_1x_1+...w_nx_n$\n",
    "$h_w=w^Tx$\n",
    "\n",
    "$J(w_1,w_0,---w_n)=\\sum 1/2m (w_0x_0^{(i)}+\\sum w_kx_k^{(i)}-y_0^{(i)})^2$<br>\n",
    "$dJ(w_1,w_0...w_n)/dw_k=\\sum 1/m (w_0x_0^{(i)}+\\sum w_kx_k^{(i)}-y_0^{(i)})x_k^{(i)}$<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
